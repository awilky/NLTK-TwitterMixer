{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK TwitterMixer \n",
    "#### by Afton Wilky\n",
    "\n",
    "The NLTK TwitterMixer generates new text based on tweets scraped from the Twitter API. \n",
    "\n",
    "Twitter data is collect by a simple search query. \n",
    "\n",
    "Writing functions scrape part of speech patterns from sentences and compile words from the text into a part of speech, python dictionary. They create new sentences by looping through the part of speech pattern and making a random choice from the part of speech dictionary.\n",
    "\n",
    "Words used in the new text can be restricted to only those which contain a specified list of phonemes (i.e. particular sounds, like the consonant 'R' and/or the vowel, 'OO'). Because the functions implement the CMU Pronouncing Dictionary, rather than regex search, differences in spelling don't affect the results (e.g. 'ER' will return both hurt, HH ER T, and heard, HH ER D).\n",
    "\n",
    "### Packages required:\n",
    "\n",
    "Natural Language Toolkit (NLTK) (including the CMU Pronouncing Dict Corpora, CMUDict) installed on your local machine (see installation / getting started instructions at http://www.nltk.org/install.html). If you are using Anaconda or Miniconda, the NLTK package for Anaconda/Miniconda must also be installed.\n",
    "\n",
    "Tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NLTK TwitterMixer, Copyright (C) 2015 Afton Wilky\n",
    "# Author: Afton Wilky <aftonwilky.com>\n",
    "# License: MIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text remixed by the TwitterMixer is scraped from the Twitter API using the Tweepy (3.5.0) module. Tweepy Documentation: http://docs.tweepy.org/en/v3.5.0/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from tweepy import API\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access Twitter API: keys and tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Tweepy module to get access to the Twitter API. Store consumer key and secret, access token and secret in a local file called 'twconfig.py'.\n",
    "\n",
    "### Instructions\n",
    "Sign up for or log into a Twitter Developer account:\n",
    "https://dev.twitter.com/\n",
    "\n",
    "Navigate to or create an app to get OAuth credentials, tokens and keys:\n",
    "https://apps.twitter.com/\n",
    "\n",
    "Access tokens and keys are availabile in the \"Manage Keys and Access Tokens.\" \n",
    "\n",
    "Create a local file 'twconfig.py'. Assign the values from your Twitter app to the following variables:\n",
    "\n",
    "consumer_key, \n",
    "\n",
    "consumer_secret, \n",
    "\n",
    "access_token, \n",
    "\n",
    "access_secret\n",
    "\n",
    "Save the local file and import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import local file that stores tokens and keys\n",
    "\n",
    "from twconfig import * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweepy documentation on using your tokens and keys:\n",
    "http://docs.tweepy.org/en/v3.5.0/auth_tutorial.html#auth-tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweepy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-b75d917191b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Get Twitter API Access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOAuthHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconsumer_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumer_secret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_access_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccess_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccess_secret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tweepy' is not defined"
     ]
    }
   ],
   "source": [
    "# Get Twitter API Access\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Twitter API and Process the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are very basic functions to query the Twitter API and store the results in variables and .txt files, using plain-text / string and Python-dict formats so the data is easy to work with.\n",
    "\n",
    "Currently, Tweepy does not support limiting searches by date or the number of results (unless they are paginated). \n",
    "\n",
    "An alternative to Tweepy is the Twython module, which is more full-featured. However, because of dependencies, (requests-mock) the package can no longer be used with Anaconda / Miniconda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#######################\n",
    "#  Query Twitter API  #\n",
    "#######################\n",
    "\n",
    "def twitter_search(query):\n",
    "    \"\"\"Query the Twitter API\"\"\"\n",
    "    tweets = []\n",
    "    for tweet in api.search(q = query):\n",
    "        tweets.append(tweet._json)\n",
    "    return tweets\n",
    "    \n",
    "    \n",
    "##################\n",
    "#  Process Data  #\n",
    "##################\n",
    "\n",
    "def process_tweets(data):\n",
    "    \"\"\"\n",
    "    Process the results of a Twitter API query.\n",
    "    Adds 'text', 'created_at' and 'hashtags' fields\n",
    "    to Python dictionary.\n",
    "    \"\"\"\n",
    "    # get the text and date fields\n",
    "    processed_data = []\n",
    "    for tweet in data:\n",
    "        entry = {}\n",
    "        entry['text'], entry['created_at'] = tweet['text'], tweet['created_at']\n",
    "        if tweet['entities']['hashtags'] != []:\n",
    "            hashtags = []\n",
    "            for hashtag in tweet['entities']['hashtags']:\n",
    "                hashtags.append(hashtag['text'])\n",
    "            entry['hashtags'] = hashtags\n",
    "        else:\n",
    "            entry['hashtags'] = []\n",
    "        processed_data.append(entry)\n",
    "    return processed_data\n",
    "\n",
    "\n",
    "def get_string(tweets):\n",
    "    \"\"\"\n",
    "    Converts unprocessed Twitter data into a string.\n",
    "    \"\"\"\n",
    "    text = ''\n",
    "    for tweet in process_tweets(tweets):\n",
    "        text = text + tweet['text'] + '\\n'\n",
    "    return text   \n",
    "\n",
    "\n",
    "##########################\n",
    "#  Write Data to Files   #\n",
    "##########################\n",
    "\n",
    "def write_file_tweets(processed_data, filepath, filename):\n",
    "    with open(filepath + filename + '.txt', 'wb') as f:\n",
    "        for tweet in processed_data:\n",
    "            f.write(bytes(tweet['text'] + '\\n', 'utf-8'))\n",
    "\n",
    "\n",
    "def write_file_hashtags(processed_data, filepath, filename):\n",
    "    with open(filepath + filename + '.txt', 'wb') as f:\n",
    "        for tweet in processed_data:\n",
    "            for hashtag in tweet['hashtags']:\n",
    "                f.write(bytes(hashtag + '\\n', 'utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call functions to create variables and files storing raw and processed Twitter data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "########################\n",
    "#  Query Twitter API   #\n",
    "########################\n",
    "\n",
    "hello_raw = twitter_search('hello')\n",
    "\n",
    "\n",
    "##############################################################\n",
    "#  Convert raw Twitter query data into a Python dictionary.  #\n",
    "##############################################################\n",
    "\n",
    "hello = process_tweets(hello_raw)\n",
    "\n",
    "\n",
    "###################################################\n",
    "#  Convert raw Twitter query data into a string   #\n",
    "#  that can be used by NLTK.                      #\n",
    "###################################################\n",
    "\n",
    "hello_text = get_string(hello_raw)\n",
    "\n",
    "\n",
    "######################################################\n",
    "#  Write processed Twitter hashtag data to a file    #                 \n",
    "#  Note: replace argument #2 with the appropriate    #\n",
    "#  filepath for your computer                        #\n",
    "######################################################\n",
    "\n",
    "# write_file_hashtags(hello, '/Users/USERNAME/', 'testing')\n",
    "\n",
    "\n",
    "######################################################\n",
    "#  Write processed Twitter tweets data to a file     #\n",
    "#  Note: replace argument #2 with the appropriate    #\n",
    "#  filepath for your computer                        #\n",
    "######################################################\n",
    "\n",
    "# write_file_tweets(hello, '/Users/USERNAME/', 'testing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Natural Language Toolkit (NLTK) provides users with access to a series of corpora and functions useful for parsing and working with text. Information about the toolkit and documentation is available at http://www.nltk.org/.\n",
    "\n",
    "#### NLTK functions require plaintext files (.txt) or variables assigned to values processed by the PlainTextCorpusReader NLTK function.\n",
    "\n",
    "e.g.\n",
    "\n",
    "tweets_all = PlaintextCorpusReader(FILEPATH, 'tweets_all\\.txt')\n",
    "\n",
    "hashtags_all = PlaintextCorpusReader(FILEPATH, 'hashtags_all\\.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## NLTK Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import nltk.data\n",
    "\n",
    "\n",
    "import random\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.corpus import CategorizedPlaintextCorpusReader\n",
    "from nltk.corpus import cmudict\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "from nltk import load_parser\n",
    "from nltk.tokenize import *\n",
    "from nltk.probability import *\n",
    "from nltk.misc.wordfinder import wordfinder\n",
    "from nltk.text import Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set variable to access the CMU Pronunciation Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prondict = nltk.corpus.cmudict.dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_file(x, filepath):\n",
    "    \"\"\"Writes a file and generates filename from first 20 characters\"\"\"\n",
    "    bad_file_chars = ['>', '<', ':', '\"', '/', '\\\\', \"|\", '*', ' ', '\\?', '\\u2014', '\\u2019']\n",
    "    filename = str(x[:20])\n",
    "    for char in bad_file_chars:\n",
    "        filename = filename.replace(char, '_')\n",
    "    with open(filepath, 'wb') as f:\n",
    "        f.write(x.encode('utf-8'))\n",
    "\n",
    "\n",
    "def write_json_file(data, filepath):\n",
    "    with open(filepath + '.json', 'w') as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "\n",
    "def read_file(filepath, filename):\n",
    "    \"\"\"Reads file\"\"\"\n",
    "    return PlaintextCorpusReader(filepath, filename)\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    sentences = nltk.sent_tokenize(text.raw())\n",
    "    words = [nltk.word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "    return words\n",
    "\n",
    "\n",
    "def process(text):\n",
    "    words = tokenize(text)\n",
    "    tagged_words = [dict(nltk.pos_tag(word)) for word in words]\n",
    "    return tagged_words     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic NLTK Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_words_longer_than(length, text):\n",
    "    \"\"\"Returns words from a text that are longer than a length.\"\"\"\n",
    "    longer_words = set([w for w in text.words() if len(w) > length])\n",
    "    print(longer_words)\n",
    "\n",
    "\n",
    "def get_words_by_char(text, ch):\n",
    "    \"\"\"Returns words in a text that contain specified character or string.\"\"\"\n",
    "    ch_words = [w.lower() for w in text.words() if ch in w]\n",
    "    return ch_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CMU Pronouncing Dictionary Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CMU Pronouncing Dictionary is a corpus of words (English) and their pronunciation, available through the Natural Language Toolkit. \n",
    "\n",
    "Pronunciations are broken into a series of 1- and 2-letter 'phones' that represent the sound of each syllable. Each word is also labeled with information about stressed and unstressed syllables (0: No stress; 1: Primary stress; 2: Secondary stress). \n",
    "\n",
    "Information about the dictionary is available at http://www.speech.cs.cmu.edu/cgi-bin/cmudict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######################################################\n",
    "#  Helper Functions - CMU Pronunciation Dictionary   #\n",
    "######################################################\n",
    "\n",
    "def get_safe_cmudict_list(text):\n",
    "    \"\"\"\n",
    "    Returns a LIST of words in a text that can be \n",
    "    processed by the CMU Pronuncation Dictionary.\n",
    "    \"\"\"\n",
    "    # get_cmudict_error_words(text)\n",
    "    cmu_dict_words = sorted([w.lower() for w in text.words() \n",
    "                             if w.lower() in prondict])\n",
    "    return cmu_dict_words\n",
    "\n",
    "\n",
    "def get_safe_cmudict_set(text):\n",
    "    \"\"\"\n",
    "    Returns a SET of words in a text that can be \n",
    "    processed by the CMU Pronuncation Dictionary.\n",
    "    \"\"\"\n",
    "    return set(get_safe_cmudict_list(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phone-based Functions (CMU Pronouncing Dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For information about 'phones', see the \"Phoneme Set\" section of the CMU Dict information page: http://www.speech.cs.cmu.edu/cgi-bin/cmudict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_phones(text):\n",
    "    \"\"\"Returns a list of CMU Dictionary phones in a text.\"\"\"\n",
    "    cmu_dict_words = get_safe_cmudict_list(text)\n",
    "    return sorted([ph for w in cmu_dict_words for ph in prondict[w][0]])\n",
    "\n",
    "\n",
    "def get_phones_by_freq(text):\n",
    "    \"\"\"Returns a list of CMU Dictionary phones in a text, sorted by frequency.\"\"\"\n",
    "    return get_by_freq(get_phones(text))\n",
    "\n",
    "\n",
    "def get_words_by_phone(phone, text):\n",
    "    \"\"\"Returns a list of words in a text that contain a specified phone.\"\"\"\n",
    "    cmu_dict_words = get_safe_cmudict_set(text)\n",
    "    return sorted(set([w.lower() for w in cmu_dict_words for \n",
    "                       ph in prondict[w][0] if phone[:2] in ph]))\n",
    "\n",
    "\n",
    "def get_phone_words(phone_list, text):\n",
    "    return set([word for phone in phone_list for \n",
    "                word in get_words_by_phone(phone, text)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Lexical / Grammatical Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_grammar(text):\n",
    "    \"\"\"return a list containing the part of speech order of a sentences in a text\"\"\"\n",
    "    processed_text = process(text)\n",
    "    grammar = OrderedDict()\n",
    "    for i in range(0, len(processed_text)):\n",
    "        grammar[i] = []\n",
    "        for k, v in processed_text[i].items():\n",
    "            # print grammar[i]\n",
    "            grammar[i].append(v)\n",
    "    return grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_pos_dict(text):\n",
    "    \"\"\"Returns a dictionary of words sorted by their part of speech.\"\"\"\n",
    "    tagged_words = process(text)\n",
    "    pos = defaultdict(set)\n",
    "    for i in range(0, len(tagged_words)):\n",
    "        for k, v in tagged_words[i].items():\n",
    "            pos[v].add(k)\n",
    "    return pos\n",
    "\n",
    "\n",
    "def make_word_pos_dict(text):\n",
    "    \"\"\"Returns a flat dict of words and parts of speach in text.\"\"\"\n",
    "    tagged_words = process(text)\n",
    "    word_pos_dict = {}\n",
    "    for i in range(0, len(tagged_words)):\n",
    "        for word, pos in tagged_words[i].items():\n",
    "            word_pos_dict[word] = pos\n",
    "    return word_pos_dict\n",
    "\n",
    "\n",
    "def make_phone_pos_dict(phone_list, text):\n",
    "    \"\"\"\n",
    "    Returns a dictionary of words containing the specified phones, \n",
    "    sorted by their part of speech\n",
    "    \"\"\"\n",
    "    pos = make_pos_dict(text)\n",
    "    phone_words = get_phone_words(phone_list, text)\n",
    "    phone_pos_dict = defaultdict(list)\n",
    "    for k, v in pos.items():\n",
    "        for word in v:\n",
    "            if word in phone_words:\n",
    "                phone_pos_dict[k].append(word)\n",
    "    return phone_pos_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_sentences(text):\n",
    "    \"\"\"\n",
    "    Writes sentences using the part of speech patterns and words \n",
    "    from the input text.\n",
    "    \"\"\"\n",
    "    # scrape part of speech patterns from input text\n",
    "    grammar = get_grammar(text)\n",
    "    # compile a dictionary, associating each word with a part of speech\n",
    "    pos_dict = make_pos_dict(text)\n",
    "    new_text = []\n",
    "    # go through each sentence in grammar\n",
    "    for i in range(0, len(grammar)): \n",
    "        # for each pos in grammar[i] make a random choice from the pos_dict\n",
    "        for pos in grammar[i]:\n",
    "            word = random.choice(list(pos_dict[pos]))\n",
    "            # append that choice to new_text\n",
    "            new_text.append(word)\n",
    "    return ' '.join(new_text)\n",
    "\n",
    "\n",
    "def get_new_text(word_dict, grammar):\n",
    "    \"\"\"\n",
    "    Writes new text based on an input grammar and dictionary of words \n",
    "    associated with their part of speech.\n",
    "    \"\"\"\n",
    "    new_text = []\n",
    "    # go through each sentence in grammar\n",
    "    for i in range(0, len(grammar)): \n",
    "        # for each pos in grammar[i] make a random choice from the phone_pos_dict\n",
    "        for pos in grammar[i]:\n",
    "            if word_dict[pos]:\n",
    "                word = random.choice(list(word_dict[pos]))\n",
    "                # append that choice to new_text\n",
    "                new_text.append(word)\n",
    "    return ' '.join(new_text)\n",
    "\n",
    "\n",
    "def write_phone_sentences(phone_list, text):\n",
    "    \"\"\"\n",
    "    Writes sentences using the part of speech patterns and words \n",
    "    from the input text that contain specified phones.\n",
    "    \"\"\"\n",
    "    grammar = get_grammar(text)\n",
    "    phone_pos_dict = make_phone_pos_dict(phone_list, text)\n",
    "    return get_new_text(phone_pos_dict, grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write new text and save it to a file, reading text from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############################\n",
    "#  Read text from .txt file  #\n",
    "##############################\n",
    "\n",
    "# new_file = read_file('FILEPATH', 'FILENAME.txt')\n",
    "\n",
    "###  OR  ###\n",
    "\n",
    "# tweets = PlaintextCorpusReader('FILEPATH', 'FILENAME.txt')\n",
    "\n",
    "\n",
    "################################################################\n",
    "# Write sentences based on the file read in the previous step  #\n",
    "################################################################\n",
    "\n",
    "# Write new sentences based on words and grammar in a text\n",
    "# new_text = write_sentences(new_file)\n",
    "\n",
    "###  OR  ###\n",
    "\n",
    "# Write new sentences with specified phones\n",
    "# based on words and grammar in a text\n",
    "# new_text = write_phone_sentences(['OW', 'OY', 'UW', 'AO'], new_file)\n",
    "\n",
    "\n",
    "#############################################\n",
    "#  Create a new file containing the         #\n",
    "#  sentences written in the previous step   #\n",
    "#############################################\n",
    "\n",
    "# write_file(new_text)\n",
    "# print(new_new_text)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
